{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Data Protection for Insurance Company Clients\n",
    "\n",
    "**Project Description**\n",
    "\n",
    "You need to protect the data of the clients of the \"Even in case of flood\" insurance company. Develop a method of data transformation so that it is difficult to restore personal information from it. Justify the correctness of its operation.\n",
    "\n",
    "The goal is to protect the data in such a way that the quality of machine learning models does not deteriorate after transformation. It is not necessary to select the best model.\n",
    "\n",
    "Work plan:\n",
    "\n",
    "1. Load and examine the data.\n",
    "2. Multiply the features by a reversible matrix. Check if the quality of linear regression changes.\n",
    "   1. It changes. Provide examples of matrices.\n",
    "   2. It does not change. Indicate how the parameters of linear regression in the original problem are related to those in the transformed one.\n",
    "3. Propose a data transformation algorithm to solve the problem. Show why the quality of linear regression will not change.\n",
    "4. Implement this algorithm by applying matrix operations. Verify that the quality of linear regression from `sklearn` does not differ before and after transformation using the `R2` metric.\n",
    "\n",
    "**Data Description**\n",
    "\n",
    "The dataset is located in the file `/datasets/insurance.csv`.\n",
    "\n",
    "1. **Features:** gender, age, insured's salary, number of family members.\n",
    "2. **Target feature:** the number of insurance claims made by the client in the last 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "\n",
    "from fast_ml import eda\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_WIDTH = 9 * 100\n",
    "FIG_HEIGHT = 5 * 100\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_claims = pd.read_csv('insurance.csv')\n",
    "except:\n",
    "    raw_claims = pd.read_csv('/datasets/insurance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Data Description\n",
    "\n",
    "Let's explore the main dependencies in the data before using them in machine learning algorithms.\n",
    "\n",
    "Summary Tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_type</th>\n",
       "      <th>data_type_grp</th>\n",
       "      <th>num_unique_values</th>\n",
       "      <th>sample_unique_values</th>\n",
       "      <th>num_missing</th>\n",
       "      <th>perc_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Пол</th>\n",
       "      <td>int64</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Возраст</th>\n",
       "      <td>float64</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>46</td>\n",
       "      <td>[41.0, 46.0, 29.0, 21.0, 28.0, 43.0, 39.0, 25....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Зарплата</th>\n",
       "      <td>float64</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>524</td>\n",
       "      <td>[49600.0, 38000.0, 21000.0, 41700.0, 26100.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Члены семьи</th>\n",
       "      <td>int64</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 0, 2, 4, 3, 5, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Страховые выплаты</th>\n",
       "      <td>int64</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 2, 3, 5, 4]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  data_type data_type_grp num_unique_values  \\\n",
       "Пол                   int64     Numerical                 2   \n",
       "Возраст             float64     Numerical                46   \n",
       "Зарплата            float64     Numerical               524   \n",
       "Члены семьи           int64     Numerical                 7   \n",
       "Страховые выплаты     int64     Numerical                 6   \n",
       "\n",
       "                                                sample_unique_values  \\\n",
       "Пол                                                           [1, 0]   \n",
       "Возраст            [41.0, 46.0, 29.0, 21.0, 28.0, 43.0, 39.0, 25....   \n",
       "Зарплата           [49600.0, 38000.0, 21000.0, 41700.0, 26100.0, ...   \n",
       "Члены семьи                                    [1, 0, 2, 4, 3, 5, 6]   \n",
       "Страховые выплаты                                 [0, 1, 2, 3, 5, 4]   \n",
       "\n",
       "                  num_missing perc_missing  \n",
       "Пол                         0          0.0  \n",
       "Возраст                     0          0.0  \n",
       "Зарплата                    0          0.0  \n",
       "Члены семьи                 0          0.0  \n",
       "Страховые выплаты           0          0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(eda.df_info(raw_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Пол</th>\n",
       "      <th>Возраст</th>\n",
       "      <th>Зарплата</th>\n",
       "      <th>Члены семьи</th>\n",
       "      <th>Страховые выплаты</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>5000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.5</td>\n",
       "      <td>30.95</td>\n",
       "      <td>39916.36</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>8.44</td>\n",
       "      <td>9900.08</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5300.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>33300.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>40200.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37.00</td>\n",
       "      <td>46600.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>65.00</td>\n",
       "      <td>79000.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Пол  Возраст  Зарплата  Члены семьи  Страховые выплаты\n",
       "count  5000.0  5000.00   5000.00      5000.00            5000.00\n",
       "mean      0.5    30.95  39916.36         1.19               0.15\n",
       "std       0.5     8.44   9900.08         1.09               0.46\n",
       "min       0.0    18.00   5300.00         0.00               0.00\n",
       "25%       0.0    24.00  33300.00         0.00               0.00\n",
       "50%       0.0    30.00  40200.00         1.00               0.00\n",
       "75%       1.0    37.00  46600.00         2.00               0.00\n",
       "max       1.0    65.00  79000.00         6.00               5.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(round(raw_claims.describe(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 30/30 [00:01<00:00, 17.19it/s, Completed]                                   \n",
      "Generate report structure: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cd8a4d90084b8b9f683a74c3b78419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ProfileReport(raw_claims).to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tidy up the dataset before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims = (\n",
    "    raw_claims\n",
    "    .copy()\n",
    "    .rename(columns={\n",
    "        'Пол': 'is_male', 'Возраст': 'age', 'Зарплата': 'income',\n",
    "        'Члены семьи': 'family_members_count', 'Страховые выплаты': 'claims_count'\n",
    "    })\n",
    "    .astype({\n",
    "        'age': 'int64', 'income': 'int64'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary observations:\n",
    "\n",
    "1. There are no missing values in the dataset, but there are duplicate rows. They should not affect the solution of the business task, so we will leave them as they are.\n",
    "\n",
    "2. The dataset is balanced with respect to the `is_male` feature, with equal representation between the two genders. This is good as it reduces the risk of bias in the data.\n",
    "\n",
    "3. The dataset covers a wide range of ages, from 18 to 65 years old with a mean value of around 31 years. This means that the data covers a broad range of clients.\n",
    "\n",
    "4. For the `claims_count` column, we can observe that the mean value is `0.15`, closer to the minimum value of `0`, and the median is also `0`. This indicates that a large number of people in the dataset do not receive insurance claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "\n",
    "Let's check what transformations we can apply to the original features without deteriorating the quality of the ML models.\n",
    "\n",
    "### Multiplication by an Invertible Matrix\n",
    "\n",
    "First, let's see what happens if we multiply the features by an invertible matrix. This task can be solved explicitly. For this purpose, let's assume that:\n",
    "\n",
    "1. $X$ - feature matrix\n",
    "2. $y$ - target vector\n",
    "3. $w$ - matrix of linear regression coefficients\n",
    "4. $w_0$ - constant coefficient\n",
    "5. $A$ - invertible matrix we multiply by\n",
    "\n",
    "Then the linear regression model (1) is:\n",
    "\n",
    "$$\n",
    "y = X \\cdot w + w_0\n",
    "$$\n",
    "\n",
    "The new feature matrix (2) is:\n",
    "\n",
    "$$\n",
    "X' = X \\cdot A\n",
    "$$\n",
    "\n",
    "And the new model (3) becomes:\n",
    "\n",
    "$$\n",
    "y = X' \\cdot w' + w_0\n",
    "$$\n",
    "\n",
    "Then substituting (2) into (3) gives (4):\n",
    "\n",
    "$$\n",
    "y = X \\cdot (Aw') + w_0\n",
    "$$\n",
    "\n",
    "Here $Aw' = w''$ - the new weight vector. This can be rewritten again as (5):\n",
    "\n",
    "$$\n",
    "y = X \\cdot w'' + w_0\n",
    "$$\n",
    "\n",
    "This means that after the transformation, it is possible to find a new weight vector $w''$ for which the prediction vector $y$ remains unchanged.\n",
    "\n",
    "Note that $w'$ is not equal to $w$, and only thanks to the relation $w'' = Aw'$ we can make the same predictions in our transformed space as in the original space.\n",
    "\n",
    "Thus, the weights of the model in the transformed feature space ($w'$) are related to the weights of the model in the original feature space ($w$) by the equation $w' = A^{-1}w$ (which we obtained by solving the equation $w'' = Aw'$ for $w'$). It is worth noting that all this is possible only because $A$ is invertible.\n",
    "\n",
    "### Implementation of Transformation\n",
    "\n",
    "The algorithm we will use is as follows:\n",
    "\n",
    "1. Generating an invertible matrix:\n",
    "    1. The matrix is generated randomly with dimensions matching the number of features in the $X$ matrix.\n",
    "    2. The matrix must be invertible, which is checked by attempting to calculate the inverse matrix. If the matrix is non-invertible, a new random matrix is generated until an invertible matrix is found.\n",
    "\n",
    "2. Data transformation:\n",
    "    1. The feature matrix $X$ is multiplied by the generated invertible matrix. This transformation does not change the target vector $y$.\n",
    "    2. The transformed features and target values are split into training and testing sets.\n",
    "\n",
    "Further work will be standard for an ML project.\n",
    "\n",
    "Let's write the functions that will perform the transformations for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_matrix(features: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    Generates an invertible transformation matrix with the same number of columns as the input features.\n",
    "\n",
    "    Parameters:\n",
    "    features (np.array): The feature matrix.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An invertible transformation matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        transform = np.random.rand(features.shape[1], features.shape[1])\n",
    "        if np.linalg.det(transform) != 0:\n",
    "            return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform_matrix(df_claims.drop('claims_count', axis=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "In the last section, let's check what we've achieved. First, let's split the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(data: pd.DataFrame, target: str, test_size=0.25):\n",
    "    \"\"\"\n",
    "    Splits input features and target into training and validation sets and\n",
    "    returns them in a dictionary format for easy access.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe with features and target.\n",
    "    target (str): The target column name.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing split data.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        data.drop(target, axis=1), data[target],\n",
    "        test_size=test_size, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    dct_splits = {\n",
    "        'train': {'features': X_train, 'target': pd.DataFrame(y_train, columns=[target])},\n",
    "        'valid': {'features': X_valid, 'target': pd.DataFrame(y_valid, columns=[target])}\n",
    "    }\n",
    "    \n",
    "    return dct_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's write a function to apply the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(dct_split: dict, transform: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Apply a transformation to the feature matrix in a split data dictionary.\n",
    "    \n",
    "    This function multiplies the feature matrix in the input dictionary by a given\n",
    "    transformation matrix using the dot product. The resulting transformed features and\n",
    "    the original target values are then returned as a new dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    dct_split (dict): A dictionary containing the data split.\n",
    "    Expected keys are 'features' and 'target' with values as numpy arrays.\n",
    "    \n",
    "    transform (numpy.ndarray): A transformation matrix that will be used\n",
    "    to transform the feature matrix by matrix multiplication.\n",
    "\n",
    "    Returns:\n",
    "    dict: A new dictionary with the same structure as the input, but with the feature\n",
    "    matrix transformed by the given transformation matrix.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'features': np.dot(dct_split['features'], transform),\n",
    "        'target': dct_split['target']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's record the original and transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_splits = {\n",
    "    'original': get_data_splits(df_claims, 'claims_count'),\n",
    "    'transformed': {\n",
    "        split: apply_transform(get_data_splits(df_claims, 'claims_count')[split], transform)\n",
    "        for split in ['train', 'valid']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create linear models and train them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    name: LinearRegression().fit(\n",
    "        dct_splits[name]['train']['features'],\n",
    "        dct_splits[name]['train']['target']\n",
    "    )\n",
    "    for name in dct_splits\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the R2 metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original R2 score: 0.42548\n",
      "transformed R2 score: 0.42548\n"
     ]
    }
   ],
   "source": [
    "for name in models:\n",
    "    print(f\"{name} R2 score: \", end=\"\")\n",
    "    print(round(\n",
    "        r2_score(\n",
    "            dct_splits[name]['valid']['target'],\n",
    "            models[name].predict(dct_splits[name]['valid']['features'])\n",
    "        ), 5\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Conclusions\n",
    "\n",
    "In this project, we explored methods for ensuring the security of client data for the insurance company \"Even Flood\". We utilized an approach based on data transformation by multiplying features by a reversible matrix.\n",
    "\n",
    "1. **Data Analysis**: Initial data analysis showed that the provided data does not contain explicit missing values or anomalies, allowing us to quickly proceed to the next steps.\n",
    "\n",
    "2. **Data Transformation**: We verified the claim that multiplying features by a reversible matrix does not change the quality of linear regression. To confirm this, we conducted a series of mathematical operations and deductions, which confirmed the claim.\n",
    "\n",
    "3. **Development of Data Transformation Algorithm**: We developed an algorithm for data transformation, which was then implemented using matrix operations.\n",
    "\n",
    "4. **Quality Check of Linear Regression**: After applying the data transformation, we trained two linear regression models: one on the original data and the other on the transformed data. Evaluating the quality of these models using the R2 metric showed that the quality of linear regression did not change after the transformation.\n",
    "\n",
    "As a result, we can conclude that the proposed method of data transformation allows us to ensure the security of clients' personal data without compromising the quality of machine learning models."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1605,
    "start_time": "2023-05-21T19:46:45.978Z"
   },
   {
    "duration": 35,
    "start_time": "2023-05-21T19:46:52.071Z"
   },
   {
    "duration": 836,
    "start_time": "2023-05-21T19:46:54.792Z"
   },
   {
    "duration": 168,
    "start_time": "2023-05-21T19:48:12.739Z"
   },
   {
    "duration": 73,
    "start_time": "2023-05-21T19:48:16.311Z"
   },
   {
    "duration": 24,
    "start_time": "2023-05-21T19:48:16.386Z"
   },
   {
    "duration": 360,
    "start_time": "2023-05-21T19:48:16.412Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-21T19:48:16.774Z"
   },
   {
    "duration": 321,
    "start_time": "2023-05-21T19:48:16.781Z"
   },
   {
    "duration": 64,
    "start_time": "2023-05-21T19:48:17.103Z"
   },
   {
    "duration": 12362,
    "start_time": "2023-05-21T19:48:22.972Z"
   },
   {
    "duration": 108,
    "start_time": "2023-05-21T19:48:35.337Z"
   },
   {
    "duration": 1385,
    "start_time": "2023-05-22T18:16:08.557Z"
   },
   {
    "duration": 37,
    "start_time": "2023-05-22T18:16:09.944Z"
   },
   {
    "duration": 865,
    "start_time": "2023-05-22T18:16:09.983Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:16:10.849Z"
   },
   {
    "duration": 119,
    "start_time": "2023-05-22T18:16:10.860Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.982Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.984Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.986Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.989Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.990Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.991Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:16:10.993Z"
   },
   {
    "duration": 2493,
    "start_time": "2023-05-22T18:16:36.511Z"
   },
   {
    "duration": 2398,
    "start_time": "2023-05-22T18:16:49.695Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:18:55.751Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:19:30.158Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-22T18:20:12.910Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:20:21.983Z"
   },
   {
    "duration": 77,
    "start_time": "2023-05-22T18:20:32.174Z"
   },
   {
    "duration": 87,
    "start_time": "2023-05-22T18:20:35.022Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-22T18:20:41.677Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-22T18:21:11.639Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:21:20.782Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-22T18:23:28.845Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-22T18:23:42.013Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-22T18:25:45.151Z"
   },
   {
    "duration": 33,
    "start_time": "2023-05-22T18:29:58.816Z"
   },
   {
    "duration": 45,
    "start_time": "2023-05-22T18:30:04.874Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:30:19.373Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-22T18:30:24.428Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:30:28.442Z"
   },
   {
    "duration": 70,
    "start_time": "2023-05-22T18:30:47.915Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:30:50.837Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-22T18:30:54.031Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-22T18:31:03.340Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:31:06.987Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:31:08.197Z"
   },
   {
    "duration": 26,
    "start_time": "2023-05-22T18:31:09.340Z"
   },
   {
    "duration": 179,
    "start_time": "2023-05-22T18:31:26.445Z"
   },
   {
    "duration": 1293,
    "start_time": "2023-05-22T18:31:50.893Z"
   },
   {
    "duration": 17,
    "start_time": "2023-05-22T18:31:52.188Z"
   },
   {
    "duration": 25,
    "start_time": "2023-05-22T18:31:52.207Z"
   },
   {
    "duration": 27,
    "start_time": "2023-05-22T18:31:52.234Z"
   },
   {
    "duration": 773,
    "start_time": "2023-05-22T18:31:52.263Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:31:53.037Z"
   },
   {
    "duration": 2404,
    "start_time": "2023-05-22T18:31:53.048Z"
   },
   {
    "duration": 184,
    "start_time": "2023-05-22T18:31:55.453Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:31:55.638Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:31:55.640Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:31:55.640Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:31:55.642Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:31:55.643Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:31:55.644Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-22T18:34:11.456Z"
   },
   {
    "duration": 25,
    "start_time": "2023-05-22T18:34:14.034Z"
   },
   {
    "duration": 2245,
    "start_time": "2023-05-22T18:34:26.018Z"
   },
   {
    "duration": 638,
    "start_time": "2023-05-22T18:34:37.931Z"
   },
   {
    "duration": 628,
    "start_time": "2023-05-22T18:34:58.575Z"
   },
   {
    "duration": 46,
    "start_time": "2023-05-22T18:35:11.934Z"
   },
   {
    "duration": 2240,
    "start_time": "2023-05-22T18:35:58.893Z"
   },
   {
    "duration": 46,
    "start_time": "2023-05-22T18:36:10.737Z"
   },
   {
    "duration": 35006,
    "start_time": "2023-05-22T18:43:11.984Z"
   },
   {
    "duration": 72,
    "start_time": "2023-05-22T18:44:58.325Z"
   },
   {
    "duration": 60,
    "start_time": "2023-05-22T18:45:29.078Z"
   },
   {
    "duration": 110,
    "start_time": "2023-05-22T18:46:32.716Z"
   },
   {
    "duration": 17,
    "start_time": "2023-05-22T18:48:06.773Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-22T18:48:19.134Z"
   },
   {
    "duration": 1344,
    "start_time": "2023-05-22T18:48:49.944Z"
   },
   {
    "duration": 22,
    "start_time": "2023-05-22T18:48:51.291Z"
   },
   {
    "duration": 16,
    "start_time": "2023-05-22T18:48:51.315Z"
   },
   {
    "duration": 26,
    "start_time": "2023-05-22T18:48:51.333Z"
   },
   {
    "duration": 711,
    "start_time": "2023-05-22T18:48:51.362Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-22T18:48:52.074Z"
   },
   {
    "duration": 2355,
    "start_time": "2023-05-22T18:48:52.085Z"
   },
   {
    "duration": 46,
    "start_time": "2023-05-22T18:48:54.442Z"
   },
   {
    "duration": 200,
    "start_time": "2023-05-22T18:48:54.489Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:48:54.691Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:48:54.693Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:48:54.694Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:48:54.695Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-22T18:48:54.696Z"
   },
   {
    "duration": 35180,
    "start_time": "2023-05-22T18:49:14.678Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-22T18:51:33.684Z"
   },
   {
    "duration": 36161,
    "start_time": "2023-05-22T18:59:42.601Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-22T19:00:25.095Z"
   },
   {
    "duration": 1322,
    "start_time": "2023-05-22T19:00:47.296Z"
   },
   {
    "duration": 1470,
    "start_time": "2023-05-22T19:01:46.489Z"
   },
   {
    "duration": 25,
    "start_time": "2023-05-22T19:02:02.352Z"
   },
   {
    "duration": 25,
    "start_time": "2023-05-22T19:02:11.960Z"
   },
   {
    "duration": 1220,
    "start_time": "2023-05-22T19:02:35.260Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-22T19:02:45.317Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-22T19:02:50.617Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-22T19:02:53.456Z"
   },
   {
    "duration": 30,
    "start_time": "2023-05-22T19:02:57.872Z"
   },
   {
    "duration": 1464,
    "start_time": "2023-05-22T19:03:02.297Z"
   },
   {
    "duration": 14,
    "start_time": "2023-05-22T19:03:10.960Z"
   },
   {
    "duration": 17,
    "start_time": "2023-05-22T19:03:28.776Z"
   },
   {
    "duration": 22,
    "start_time": "2023-05-22T19:03:33.560Z"
   },
   {
    "duration": 118,
    "start_time": "2023-05-22T19:03:46.106Z"
   },
   {
    "duration": 194,
    "start_time": "2023-05-22T19:05:43.456Z"
   },
   {
    "duration": 223,
    "start_time": "2023-05-22T19:06:06.752Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-22T19:06:09.856Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-22T19:08:23.155Z"
   },
   {
    "duration": 48,
    "start_time": "2023-05-22T19:08:27.164Z"
   },
   {
    "duration": 206,
    "start_time": "2023-05-22T19:08:48.168Z"
   },
   {
    "duration": 21,
    "start_time": "2023-05-22T19:08:51.352Z"
   },
   {
    "duration": 50,
    "start_time": "2023-05-22T19:09:09.387Z"
   },
   {
    "duration": 79,
    "start_time": "2023-05-22T19:11:42.417Z"
   },
   {
    "duration": 42,
    "start_time": "2023-05-22T19:11:44.002Z"
   },
   {
    "duration": 98,
    "start_time": "2023-05-22T19:11:52.163Z"
   },
   {
    "duration": 42,
    "start_time": "2023-05-22T19:11:53.777Z"
   },
   {
    "duration": 122,
    "start_time": "2023-05-22T19:12:02.124Z"
   },
   {
    "duration": 43,
    "start_time": "2023-05-22T19:12:03.660Z"
   },
   {
    "duration": 134,
    "start_time": "2023-05-22T19:12:12.560Z"
   },
   {
    "duration": 49,
    "start_time": "2023-05-22T19:12:13.873Z"
   },
   {
    "duration": 155,
    "start_time": "2023-05-22T19:12:23.064Z"
   },
   {
    "duration": 48,
    "start_time": "2023-05-22T19:12:25.472Z"
   },
   {
    "duration": 204,
    "start_time": "2023-05-22T19:12:41.581Z"
   },
   {
    "duration": 55,
    "start_time": "2023-05-22T19:12:43.242Z"
   },
   {
    "duration": 2488,
    "start_time": "2023-05-22T19:12:51.360Z"
   },
   {
    "duration": 41,
    "start_time": "2023-05-22T19:12:54.281Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-22T19:13:13.504Z"
   },
   {
    "duration": 131,
    "start_time": "2023-05-22T19:13:36.504Z"
   },
   {
    "duration": 46,
    "start_time": "2023-05-22T19:13:38.017Z"
   },
   {
    "duration": 15,
    "start_time": "2023-05-22T19:14:18.176Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-22T19:14:23.481Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-22T19:14:25.159Z"
   },
   {
    "duration": 22,
    "start_time": "2023-05-22T19:14:26.767Z"
   },
   {
    "duration": 68,
    "start_time": "2023-05-22T19:14:31.816Z"
   },
   {
    "duration": 49,
    "start_time": "2023-05-22T19:14:40.818Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "95633e14b7ea774968f538ab8687a60d358136f4a990a7903a22cc539dc1dcdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
